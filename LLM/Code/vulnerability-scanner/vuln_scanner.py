#!/usr/bin/env python3
"""
Typer-based Asynchronous Python Web Vulnerability Scanner
---------------------------------------------------------

Installation:
-------------
1. python -m venv .venv
2. source .venv/bin/activate (or .venv\Scripts\activate on Windows)
3. pip install typer aiohttp beautifulsoup4 playwright
4. playwright install  (if using --enable-dom-xss)
5. python vuln_scanner.py scan

Features:
---------
1. Asynchronous crawling with asyncio + aiohttp (queue-based).
2. SQL Injection testing, reflected XSS, and optional DOM-based XSS via Playwright.
3. CSRF checks with warning when tokens are missing.
4. Max depth argument to limit crawling depth.
5. Exclusion list for skipping certain paths or URLs.
6. Custom user-agent setting.
7. Real-time logging via Typer outputs.
8. Interactive prompts for all configuration parameters.

Disclaimer:
-----------
Use only on targets you have explicit permission to scan.
Unauthorized scanning may be illegal.
"""

import asyncio
import aiohttp
import re
import json
from typing import Optional, List, Set, Dict, Tuple
from urllib.parse import urljoin, urlparse

import typer
from bs4 import BeautifulSoup

# For optional DOM-based XSS detection
try:
    from playwright.async_api import async_playwright
except ImportError:
    async_playwright = None


# --------------------- Payloads & Signatures ----------------------
SQL_PAYLOADS = [
    "' OR 1=1 --",
    "' UNION SELECT NULL,NULL,NULL --",
    "'; DROP TABLE users; --",
    '" OR 1=1 --',
    "admin' --",
]

XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "\"/><script>alert('XSS')</script>",
    "'\"><img src=x onerror=alert('XSS')>",
]

OTHER_PAYLOADS = [
    # LDAP injection, command injection, SSRF placeholders, etc.
    "*)(|(objectClass=*))",  # Example LDAP injection
    "|| ping -c 4 127.0.0.1 ||",  # Example command injection
]

SQL_ERROR_SIGNATURES = [
    "You have an error in your SQL syntax",
    "Warning: mysql_",
    "Warning: pg_",
    "Unclosed quotation mark after the character string",
    "SQL syntax error",
]


class AsyncWebVulnerabilityScanner:
    """
    Asynchronous web vulnerability scanner with BFS-based crawling,
    injection payload testing, optional DOM-based XSS checks, and more.
    """

    def __init__(
        self,
        base_url: str,
        max_concurrency: int = 5,
        enable_dom_xss: bool = False,
        disable_csrf_check: bool = False,
        max_depth: int = 3,
        exclude_urls: Optional[List[str]] = None,
        user_agent: Optional[str] = None,
        log_callback=None,
    ) -> None:
        """
        Args:
            base_url: The starting URL for scanning.
            max_concurrency: Limit for concurrent aiohttp requests.
            enable_dom_xss: Whether to check for DOM-based XSS with Playwright.
            disable_csrf_check: If True, CSRF checks will be skipped.
            max_depth: Maximum crawl depth.
            exclude_urls: List of substrings to exclude from crawling.
            user_agent: Custom User-Agent header.
            log_callback: A callable that takes a str to log messages (e.g., typer.echo).
        """
        self.base_url = base_url
        self.max_concurrency = max_concurrency
        self.semaphore = asyncio.Semaphore(max_concurrency)

        self.enable_dom_xss = enable_dom_xss
        self.disable_csrf_check = disable_csrf_check
        self.max_depth = max_depth

        self.exclude_urls = [p.lower() for p in (exclude_urls or [])]
        self.headers = {}
        if user_agent:
            self.headers["User-Agent"] = user_agent

        self.visited_urls: Set[str] = set()
        self.results: List[str] = []

        # Queue for BFS-based scanning: holds (url, depth)
        self.url_queue: asyncio.Queue[Tuple[str, int]] = asyncio.Queue()
        self.url_queue.put_nowait((base_url, 0))

        self.append_log = log_callback or (lambda msg: None)

    def is_same_domain(self, url: str) -> bool:
        """Check if `url` is within the same domain as `self.base_url`."""
        return urlparse(url).netloc == urlparse(self.base_url).netloc

    def should_exclude(self, url: str) -> bool:
        """Return True if the URL should be excluded based on patterns."""
        return any(pattern in url.lower() for pattern in self.exclude_urls)

    async def fetch(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str = "GET",
        data: Optional[Dict[str, str]] = None,
        timeout: int = 10,
    ) -> Tuple[Optional[str], Optional[int]]:
        """
        Fetch URL using aiohttp, with concurrency managed by semaphore.
        Returns (text, status_code); (None, None) on failure.
        """
        async with self.semaphore:
            try:
                if method.upper() == "POST":
                    async with session.post(url, data=data, timeout=timeout) as resp:
                        text = await resp.text()
                        return text, resp.status
                else:
                    async with session.get(url, params=data, timeout=timeout) as resp:
                        text = await resp.text()
                        return text, resp.status
            except Exception as e:
                self.append_log(f"[DEBUG] Error fetching {url}: {e}")
                return None, None

    async def crawl_url(
        self, session: aiohttp.ClientSession, url: str, depth: int
    ) -> None:
        """Crawl a single URL: parse forms, test vulnerabilities, discover new links."""
        if url in self.visited_urls:
            return
        self.visited_urls.add(url)

        self.append_log(f"[*] Crawling (depth={depth}): {url}")
        html, status = await self.fetch(session, url)
        if not html or status != 200:
            self.append_log(f"[DEBUG] Skipping URL {url}, status={status}")
            return

        soup = BeautifulSoup(html, "html.parser")

        # 1. Analyze forms & test for vulnerabilities
        await self.test_forms(session, url, soup)

        # 2. CSRF checks if not disabled
        if not self.disable_csrf_check:
            self.check_csrf_tokens(url, soup)

        # 3. Discover new links if within depth limit
        if depth < self.max_depth:
            self.discover_links(soup, url, depth + 1)

    def discover_links(
        self, soup: BeautifulSoup, base_url: str, next_depth: int
    ) -> None:
        """Discover and enqueue new links from the page for further crawling."""
        for link in soup.find_all("a", href=True):
            full_link = urljoin(base_url, link["href"])
            if (
                self.is_same_domain(full_link)
                and not self.should_exclude(full_link)
                and full_link not in self.visited_urls
            ):
                self.append_log(
                    f"[DEBUG] Discovered link: {full_link} (depth={next_depth})"
                )
                self.url_queue.put_nowait((full_link, next_depth))

    def check_csrf_tokens(self, page_url: str, soup: BeautifulSoup) -> None:
        """
        Basic check for CSRF tokens in forms:
        - Looks for hidden inputs with 'csrf' in their name attribute.
        """
        for form in soup.find_all("form"):
            hidden_inputs = form.find_all("input", {"type": "hidden"})
            csrf_tokens = [
                inp
                for inp in hidden_inputs
                if re.search("csrf", inp.get("name", ""), re.IGNORECASE)
            ]
            if not csrf_tokens:
                msg = f"[!] Potential missing CSRF token on {page_url} (action={form.get('action')})"
                self.append_log(msg)
                self.results.append(msg)

    async def test_forms(
        self,
        session: aiohttp.ClientSession,
        page_url: str,
        soup: BeautifulSoup,
    ) -> None:
        """
        Extract forms, gather inputs, and submit payloads for SQLi, XSS, and other attacks.
        """
        for form in soup.find_all("form"):
            action = form.get("action") or ""
            method = form.get("method", "get").upper()
            target_url = urljoin(page_url, action)

            inputs = form.find_all(["input", "textarea"])
            input_names = [inp.get("name") for inp in inputs if inp.get("name")]

            # Test each category of payload
            for category, payloads in [
                ("SQL Injection", SQL_PAYLOADS),
                ("XSS", XSS_PAYLOADS),
                ("Other Injection", OTHER_PAYLOADS),
            ]:
                for payload in payloads:
                    form_data = {name: payload for name in input_names}
                    await self.submit_form(
                        session, target_url, method, form_data, category
                    )

    async def submit_form(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str,
        data: Dict[str, str],
        vuln_type: str,
    ) -> None:
        """Submit a form with a payload and perform checks for vulnerability signatures."""
        html, status = await self.fetch(session, url, method=method, data=data)
        if not html or status is None:
            return

        if vuln_type == "SQL Injection":
            for signature in SQL_ERROR_SIGNATURES:
                if signature.lower() in html.lower():
                    msg = f"[!] Possible SQL Injection at {url} with payload={data}"
                    self.append_log(msg)
                    self.results.append(msg)
                    return

        elif vuln_type == "XSS":
            # Check if payload is reflected in the response
            for val in data.values():
                if val in html:
                    msg = f"[!] Possible Reflected XSS at {url} with payload='{val}'"
                    self.append_log(msg)
                    self.results.append(msg)
                    return

    async def detect_dom_xss(self) -> None:
        """
        Use Playwright to detect DOM-based XSS by injecting payloads as query parameters.
        """
        if not async_playwright:
            self.append_log(
                "[ERROR] Playwright is not installed. Skipping DOM-based XSS."
            )
            return

        candidate_urls = [u for u in self.visited_urls if self.is_same_domain(u)]
        self.append_log("[+] Starting DOM-based XSS detection...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()

            for url in candidate_urls:
                for payload in XSS_PAYLOADS:
                    test_url = self.append_payload_to_url(url, payload)
                    self.append_log(f"[DEBUG] Testing DOM-based XSS on {test_url}")

                    page = await context.new_page()

                    async def on_dialog(dialog):
                        msg = f"[!] Possible DOM XSS at {test_url}. Dialog text: {dialog.message}"
                        self.append_log(msg)
                        self.results.append(msg)
                        await dialog.dismiss()

                    page.on("dialog", on_dialog)

                    try:
                        await page.goto(test_url, timeout=10000)
                        # Let scripts run
                        await asyncio.sleep(2)
                    except Exception as e:
                        self.append_log(
                            f"[DEBUG] DOM XSS test failed at {test_url}: {e}"
                        )
                    finally:
                        await page.close()

            await context.close()
            await browser.close()

    @staticmethod
    def append_payload_to_url(url: str, payload: str) -> str:
        """Append an XSS payload as a query parameter for DOM XSS testing."""
        if "?" in url:
            return f"{url}&xss={payload}"
        return f"{url}?xss={payload}"

    async def run(self) -> List[str]:
        """
        Main scan loop: connect, crawl all pages in BFS manner, optionally check DOM XSS.
        Returns the list of discovered vulnerabilities.
        """
        conn = aiohttp.TCPConnector(ssl=False)  # Optionally ignore SSL for testing
        async with aiohttp.ClientSession(
            connector=conn, headers=self.headers
        ) as session:
            while not self.url_queue.empty():
                url, depth = await self.url_queue.get()
                await self.crawl_url(session, url, depth)

        if self.enable_dom_xss:
            await self.detect_dom_xss()

        return self.results


# --------------------- Typer CLI App ----------------------
app = typer.Typer(help="Asynchronous Python Web Vulnerability Scanner (Typer-based)")


@app.command()
def scan(
    base_url: str = typer.Option(
        ...,
        prompt="Enter the target URL (e.g. https://example.com)",
        help="The starting URL to scan",
    ),
    max_concurrency: int = typer.Option(
        5,
        prompt="Max concurrency?",
        help="Number of concurrent requests to send at once",
    ),
    enable_dom_xss: bool = typer.Option(
        False,
        prompt="Enable DOM-based XSS checks?",
        help="If True, uses Playwright to detect DOM XSS",
    ),
    disable_csrf_check: bool = typer.Option(
        False,
        prompt="Disable CSRF checks?",
        help="If True, CSRF token checks are skipped",
    ),
    max_depth: int = typer.Option(
        3,
        prompt="Max crawl depth?",
        help="Limits how deep the crawler will follow links",
    ),
    exclude_patterns: str = typer.Option(
        "",
        prompt="Comma-separated URL substrings to exclude? (leave blank if none)",
        help="Exclude URLs containing any of these substrings",
    ),
    user_agent: str = typer.Option(
        "",
        prompt="Custom User-Agent? (leave blank for default)",
        help="Specify a custom User-Agent header",
    ),
):
    """
    Launches an asynchronous vulnerability scan of the target web application.
    """
    # Convert comma-separated excludes to a list
    exclude_list = []
    if exclude_patterns.strip():
        exclude_list = [x.strip() for x in exclude_patterns.split(",") if x.strip()]

    def log_callback(msg: str) -> None:
        # Print logs to console for real-time feedback
        typer.echo(msg)

    # Basic validation
    if not base_url.lower().startswith("http"):
        typer.echo("[ERROR] Please provide a valid URL starting with http or https.")
        raise typer.Exit(code=1)

    typer.echo(f"[INFO] Starting scan for: {base_url}")
    typer.echo(f"       Concurrency: {max_concurrency}")
    typer.echo(f"       Max Depth: {max_depth}")
    typer.echo(f"       Enable DOM XSS: {enable_dom_xss}")
    typer.echo(f"       Disable CSRF Checks: {disable_csrf_check}")
    typer.echo(f"       Exclude Patterns: {exclude_list}")
    if user_agent:
        typer.echo(f"       Custom User-Agent: {user_agent}")

    scanner = AsyncWebVulnerabilityScanner(
        base_url=base_url,
        max_concurrency=max_concurrency,
        enable_dom_xss=enable_dom_xss,
        disable_csrf_check=disable_csrf_check,
        max_depth=max_depth,
        exclude_urls=exclude_list,
        user_agent=user_agent if user_agent else None,
        log_callback=log_callback,
    )

    # Run the asynchronous scanner
    vulnerabilities = asyncio.run(scanner.run())

    typer.echo("[INFO] Scan complete. Results:\n")

    if vulnerabilities:
        for finding in vulnerabilities:
            typer.echo(f"  - {finding}")
    else:
        typer.echo("No major findings reported.")


if __name__ == "__main__":
    app()
