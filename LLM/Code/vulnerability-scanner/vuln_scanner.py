"""
Asynchronous Python Web Vulnerability Scanner
---------------------------------------------
Enhanced Version with Additional Features and Improvements

Installation / Set-up:
----------------------
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
playwright install

Usage Example:
--------------
python vuln_scanner.py --url https://example.com --max-concurrency 10 --enable-dom-xss --log-level DEBUG --max-depth 3 --exclude-urls /logout,/static --user-agent "CustomAgent/1.0" --json-out results.json

Features:
------------------------
1. Asynchronous crawling with asyncio + aiohttp (queue-based).
2. SQL Injection testing, reflected XSS, and optional DOM-based XSS via Playwright.
3. CSRF checks with warning when tokens are missing.
4. Max depth argument to limit crawling depth.
5. Exclusion list for skipping certain paths or URLs.
6. Custom user-agent setting.
7. Optional JSON output of findings.

Disclaimer:
-----------
Use only on targets you have explicit permission to scan.
Unauthorized scanning may be illegal.
"""

import asyncio
import aiohttp
import logging
import argparse
import re
import json
from typing import Optional, List, Set, Dict, Tuple
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup

# For DOM-based XSS detection (optional use if --enable-dom-xss is set)
try:
    from playwright.async_api import async_playwright
except ImportError:
    async_playwright = None


# --------------------- Payloads & Signatures ----------------------
SQL_PAYLOADS = [
    "' OR 1=1 --",
    "' UNION SELECT NULL,NULL,NULL --",
    "'; DROP TABLE users; --",
    '" OR 1=1 --',
    "admin' --",
]

XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "\"/><script>alert('XSS')</script>",
    "'\"><img src=x onerror=alert('XSS')>",
]

OTHER_PAYLOADS = [
    # LDAP injection, command injection, SSRF placeholders, etc.
    "*)(|(objectClass=*))",  # Example LDAP injection
    "|| ping -c 4 127.0.0.1 ||",  # Example command injection
]

SQL_ERROR_SIGNATURES = [
    "You have an error in your SQL syntax",
    "Warning: mysql_",
    "Warning: pg_",
    "Unclosed quotation mark after the character string",
    "SQL syntax error",
]


class AsyncWebVulnerabilityScanner:
    """
    An enhanced asynchronous web vulnerability scanner.

    Attributes:
        base_url (str): The starting URL for scanning.
        max_concurrency (int): Limit for concurrent aiohttp requests.
        enable_dom_xss (bool): Whether to check for DOM-based XSS with Playwright.
        disable_csrf_check (bool): If True, CSRF checks will be skipped.
        max_depth (int): Maximum crawl depth from the base URL.
        exclude_urls (List[str]): List of patterns or substrings to exclude from crawling.
        user_agent (str): Custom User-Agent header to use.
        results (List[str]): A list of vulnerability findings.
    """

    def __init__(
        self,
        base_url: str,
        max_concurrency: int = 5,
        enable_dom_xss: bool = False,
        disable_csrf_check: bool = False,
        max_depth: int = 3,
        exclude_urls: Optional[List[str]] = None,
        user_agent: Optional[str] = None,
    ) -> None:
        self.base_url = base_url
        self.max_concurrency = max_concurrency
        self.semaphore = asyncio.Semaphore(max_concurrency)

        self.enable_dom_xss = enable_dom_xss
        self.disable_csrf_check = disable_csrf_check
        self.max_depth = max_depth

        # Lowercase for easy substring checks
        self.exclude_urls = [p.lower() for p in (exclude_urls or [])]

        # Prepare custom headers if user agent is specified
        self.headers = {}
        if user_agent:
            self.headers["User-Agent"] = user_agent

        # Tracking
        self.visited_urls: Set[str] = set()
        self.results: List[str] = []

        # Queue for BFS-based scanning: holds tuples of (url, depth)
        self.url_queue: asyncio.Queue[Tuple[str, int]] = asyncio.Queue()
        self.url_queue.put_nowait((base_url, 0))

    def is_same_domain(self, url: str) -> bool:
        """
        Check if `url` is within the same domain as `self.base_url`.
        """
        return urlparse(url).netloc == urlparse(self.base_url).netloc

    def should_exclude(self, url: str) -> bool:
        """
        Returns True if the URL should be excluded based on user-provided patterns.
        """
        lower_url = url.lower()
        return any(pattern in lower_url for pattern in self.exclude_urls)

    async def fetch(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str = "GET",
        data: Optional[Dict[str, str]] = None,
        timeout: int = 10,
    ) -> Tuple[Optional[str], Optional[int]]:
        """
        Fetch URL using aiohttp, with concurrency managed by semaphore.

        Returns:
            text (str or None): Response text if successful, None otherwise.
            status_code (int or None): Response status code, None on error or exception.
        """
        async with self.semaphore:
            try:
                if method.upper() == "POST":
                    async with session.post(
                        url, data=data, timeout=timeout
                    ) as response:
                        resp_text = await response.text()
                        return resp_text, response.status
                else:
                    async with session.get(
                        url, params=data, timeout=timeout
                    ) as response:
                        resp_text = await response.text()
                        return resp_text, response.status
            except Exception as e:
                logging.debug(f"Error fetching {url}: {e}")
                return None, None

    async def crawl_url(
        self, session: aiohttp.ClientSession, url: str, depth: int
    ) -> None:
        """
        Crawl a single URL: parse forms, test vulnerabilities, discover new links.

        Args:
            session (aiohttp.ClientSession): The active HTTP session.
            url (str): The URL to crawl.
            depth (int): Current crawl depth.
        """
        if url in self.visited_urls:
            return
        self.visited_urls.add(url)

        logging.info(f"[*] Crawling (depth={depth}): {url}")

        html, status = await self.fetch(session, url)
        if not html or status != 200:
            logging.debug(f"Skipping URL {url}, status={status}")
            return

        soup = BeautifulSoup(html, "html.parser")

        # 1. Analyze forms & test for vulnerabilities
        await self.test_forms(session, url, soup)

        # 2. Perform CSRF checks unless disabled
        if not self.disable_csrf_check:
            self.check_csrf_tokens(url, soup)

        # 3. Discover new links if within depth limit
        if depth < self.max_depth:
            self.discover_links(soup, url, depth + 1)

    def discover_links(
        self, soup: BeautifulSoup, base_url: str, next_depth: int
    ) -> None:
        """
        Discover and enqueue new links from the page for further crawling.

        Args:
            soup (BeautifulSoup): Parsed HTML content.
            base_url (str): URL of the current page to resolve relative links.
            next_depth (int): The depth for the newly discovered URLs.
        """
        for link in soup.find_all("a", href=True):
            full_link = urljoin(base_url, link["href"])

            # Basic checks: same domain, not excluded, and not yet visited
            if (
                self.is_same_domain(full_link)
                and not self.should_exclude(full_link)
                and full_link not in self.visited_urls
            ):
                logging.debug(f"Discovered link: {full_link} (depth={next_depth})")
                self.url_queue.put_nowait((full_link, next_depth))

    def check_csrf_tokens(self, page_url: str, soup: BeautifulSoup) -> None:
        """
        Basic check for CSRF tokens in forms:
        - Looks for hidden inputs with 'csrf' in their name attribute.
        - If missing, logs a potential issue.

        Args:
            page_url (str): The page URL where forms were discovered.
            soup (BeautifulSoup): Parsed HTML of the page.
        """
        forms = soup.find_all("form")
        for form in forms:
            hidden_inputs = form.find_all("input", {"type": "hidden"})
            # Typically names like "csrf_token", "csrftoken", "token", etc.
            csrf_tokens = [
                inp
                for inp in hidden_inputs
                if re.search("csrf", inp.get("name", ""), re.IGNORECASE)
            ]
            if not csrf_tokens:
                msg = f"[!] Potential missing CSRF token on {page_url} (form action: {form.get('action')})"
                logging.warning(msg)
                self.results.append(msg)

    async def test_forms(
        self, session: aiohttp.ClientSession, page_url: str, soup: BeautifulSoup
    ) -> None:
        """
        Extract forms, gather inputs, and submit payloads for SQLi, XSS, and other injections.

        Args:
            session (aiohttp.ClientSession): The current HTTP session.
            page_url (str): URL of the page containing the forms.
            soup (BeautifulSoup): Parsed HTML content.
        """
        for form in soup.find_all("form"):
            action = form.get("action") or ""
            method = form.get("method", "get").upper()
            target_url = urljoin(page_url, action)

            inputs = form.find_all(["input", "textarea"])
            input_names = [inp.get("name") for inp in inputs if inp.get("name")]

            # Test each category of payload
            for category, payloads in [
                ("SQL Injection", SQL_PAYLOADS),
                ("XSS", XSS_PAYLOADS),
                ("Other Injection", OTHER_PAYLOADS),
            ]:
                for payload in payloads:
                    form_data = {name: payload for name in input_names}
                    await self.submit_form(
                        session, target_url, method, form_data, category
                    )

    async def submit_form(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str,
        data: Dict[str, str],
        vuln_type: str,
    ) -> None:
        """
        Submit form with a given payload and perform quick checks for vulnerability signatures.

        Args:
            session (aiohttp.ClientSession): The current HTTP session.
            url (str): Form target URL.
            method (str): HTTP method (GET/POST).
            data (Dict[str, str]): Form data/payloads.
            vuln_type (str): The category of vulnerability being tested.
        """
        html, status = await self.fetch(session, url, method=method, data=data)
        if not html or status is None:
            return

        if vuln_type == "SQL Injection":
            for signature in SQL_ERROR_SIGNATURES:
                if signature.lower() in html.lower():
                    msg = f"[!] Possible SQL Injection at {url} with payload={data}"
                    logging.warning(msg)
                    self.results.append(msg)
                    return

        elif vuln_type == "XSS":
            # Check if payload is reflected in the response (simple check)
            for val in data.values():
                if val in html:
                    msg = f"[!] Possible Reflected XSS at {url} with payload='{val}'"
                    logging.warning(msg)
                    self.results.append(msg)
                    return

        elif vuln_type == "Other Injection":
            # Placeholder for specialized detection (LDAP, command injection, SSRF, etc.)
            pass

    async def detect_dom_xss(self) -> None:
        """
        Use Playwright to detect DOM-based XSS by injecting payloads as query parameters.
        Requires Playwright to be installed and available.
        """
        if not async_playwright:
            logging.error("Playwright is not installed. Cannot detect DOM-based XSS.")
            return

        candidate_urls = [u for u in self.visited_urls if self.is_same_domain(u)]
        logging.info("[+] Starting DOM-based XSS detection with Playwright...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()

            for url in candidate_urls:
                for payload in XSS_PAYLOADS:
                    test_url = self.append_payload_to_url(url, payload)
                    logging.debug(f"Testing DOM-based XSS on {test_url}")

                    page = await context.new_page()

                    try:
                        # Intercept dialogs (alert/confirm/prompt)
                        async def on_dialog(dialog):
                            msg = f"[!] Possible DOM XSS at {test_url}. Dialog text: {dialog.message}"
                            logging.warning(msg)
                            self.results.append(msg)
                            await dialog.dismiss()

                        page.on("dialog", on_dialog)
                        await page.goto(test_url, timeout=10000)
                        # Wait for a moment to let scripts run
                        await asyncio.sleep(2)

                    except Exception as e:
                        logging.debug(f"DOM XSS test failed at {test_url}: {e}")
                    finally:
                        await page.close()

            await context.close()
            await browser.close()

    @staticmethod
    def append_payload_to_url(url: str, payload: str) -> str:
        """
        Helper to append an XSS payload as a query parameter for DOM XSS testing.
        """
        if "?" in url:
            return f"{url}&xss={payload}"
        return f"{url}?xss={payload}"

    async def run(self) -> None:
        """
        Main scan loop:
        1. Create an aiohttp.ClientSession with optional custom headers.
        2. Continuously process all URLs in the queue until empty.
        3. Optionally run DOM-based XSS detection.
        4. Print results.
        """
        conn = aiohttp.TCPConnector(ssl=False)  # Optionally ignore SSL for testing
        async with aiohttp.ClientSession(
            connector=conn, headers=self.headers
        ) as session:
            while not self.url_queue.empty():
                url, depth = await self.url_queue.get()
                await self.crawl_url(session, url, depth)

        if self.enable_dom_xss:
            await self.detect_dom_xss()

        # Print final results
        self.print_results()

    def print_results(self) -> None:
        """
        Print all discovered vulnerabilities at the end of the scan.
        """
        print("\n[+] Scan complete. Found issues:")
        if not self.results:
            print(" - No major findings.")
        else:
            for res in self.results:
                print(f" - {res}")


# --------------------- CLI & Main ----------------------
def parse_args() -> argparse.Namespace:
    """
    Parse command-line arguments and return the Namespace object.
    """
    parser = argparse.ArgumentParser(
        description="Enhanced Asynchronous Python Web Vulnerability Scanner"
    )
    parser.add_argument(
        "--url",
        required=True,
        help="Starting URL to scan (must include http or https).",
    )
    parser.add_argument(
        "--max-concurrency",
        type=int,
        default=5,
        help="Maximum number of concurrent requests (default=5).",
    )
    parser.add_argument(
        "--enable-dom-xss",
        action="store_true",
        help="Enable DOM-based XSS detection using Playwright.",
    )
    parser.add_argument(
        "--disable-csrf-check",
        action="store_true",
        help="Disable CSRF token checks on forms.",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        help="Set logging level (DEBUG, INFO, WARNING, ERROR).",
    )
    parser.add_argument(
        "--max-depth",
        type=int,
        default=3,
        help="Maximum crawl depth from the starting URL (default=3).",
    )
    parser.add_argument(
        "--exclude-urls",
        default="",
        help="Comma-separated list of URL substrings to exclude from crawling.",
    )
    parser.add_argument(
        "--user-agent",
        default=None,
        help="Custom User-Agent string for requests (optional).",
    )
    parser.add_argument(
        "--json-out",
        default=None,
        help="Save results to a JSON file instead of just printing to console.",
    )
    return parser.parse_args()


def main() -> None:
    """
    Entry point for the vulnerability scanner CLI.
    """
    args = parse_args()

    # Setup logging
    numeric_level = getattr(logging, args.log_level.upper(), logging.INFO)
    logging.basicConfig(
        level=numeric_level,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%H:%M:%S",
    )

    # Prepare exclusion list
    exclude_list = []
    if args.exclude_urls:
        exclude_list = [x.strip() for x in args.exclude_urls.split(",") if x.strip()]

    scanner = AsyncWebVulnerabilityScanner(
        base_url=args.url,
        max_concurrency=args.max_concurrency,
        enable_dom_xss=args.enable_dom_xss,
        disable_csrf_check=args.disable_csrf_check,
        max_depth=args.max_depth,
        exclude_urls=exclude_list,
        user_agent=args.user_agent,
    )

    asyncio.run(scanner.run())

    # Optionally save JSON output
    if args.json_out:
        with open(args.json_out, "w", encoding="utf-8") as f:
            json.dump(scanner.results, f, indent=2)
        logging.info(f"[+] Results saved to {args.json_out}")


if __name__ == "__main__":
    main()
