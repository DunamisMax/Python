#!/usr/bin/env python3
"""
Typer + curses-based Asynchronous Python Web Vulnerability Scanner
------------------------------------------------------------------

Usage:
------
1. Default TUI Mode (no args):  ./vuln_scanner.py
2. Explicitly launch TUI:       ./vuln_scanner.py tui
3. CLI Mode example:            ./vuln_scanner.py scan

Features:
---------
1. Asynchronous crawling with asyncio + aiohttp (queue-based).
2. SQL Injection testing, reflected XSS, and optional DOM-based XSS via Playwright.
3. CSRF checks with warning when tokens are missing.
4. Max depth argument to limit crawling depth.
5. Exclusion list for skipping certain paths or URLs.
6. Custom user-agent setting.
7. Real-time logging to console.
8. Interactive TUI for easy parameter input (curses).
9. Rich-powered CLI output for colorization and improved readability.

Disclaimer:
-----------
Use only on targets you have explicit permission to scan.
Unauthorized scanning may be illegal.
"""

import sys
import asyncio
import aiohttp
import re
import json
from typing import Optional, List, Set, Dict, Tuple
from urllib.parse import urljoin, urlparse
import curses

import typer
from rich import print as rprint
from rich.console import Console
from bs4 import BeautifulSoup

# For optional DOM-based XSS detection
try:
    from playwright.async_api import async_playwright
except ImportError:
    async_playwright = None

# -----------------------------------
# Payloads & Injection Signatures
# -----------------------------------
SQL_PAYLOADS = [
    "' OR 1=1 --",
    "' UNION SELECT NULL,NULL,NULL --",
    "'; DROP TABLE users; --",
    '" OR 1=1 --',
    "admin' --",
]

XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "\"/><script>alert('XSS')</script>",
    "'\"><img src=x onerror=alert('XSS')>",
]

OTHER_PAYLOADS = [
    "*)(|(objectClass=*))",  # Example LDAP injection
    "|| ping -c 4 127.0.0.1 ||",  # Example command injection
]

SQL_ERROR_SIGNATURES = [
    "You have an error in your SQL syntax",
    "Warning: mysql_",
    "Warning: pg_",
    "Unclosed quotation mark after the character string",
    "SQL syntax error",
]


class AsyncWebVulnerabilityScanner:
    """
    Asynchronous web vulnerability scanner with BFS-based crawling,
    injection payload testing, optional DOM-based XSS checks, and more.
    """

    def __init__(
        self,
        base_url: str,
        max_concurrency: int = 5,
        enable_dom_xss: bool = False,
        disable_csrf_check: bool = False,
        max_depth: int = 3,
        exclude_urls: Optional[List[str]] = None,
        user_agent: Optional[str] = None,
        log_callback=None,
    ) -> None:
        """
        Args:
            base_url: The starting URL for scanning.
            max_concurrency: Limit for concurrent aiohttp requests.
            enable_dom_xss: Whether to check for DOM-based XSS with Playwright.
            disable_csrf_check: If True, CSRF checks will be skipped.
            max_depth: Maximum crawl depth.
            exclude_urls: List of substrings to exclude from crawling.
            user_agent: Custom User-Agent header.
            log_callback: A callable that takes a str to log messages (e.g., console.print).
        """
        self.base_url = base_url
        self.max_concurrency = max_concurrency
        self.semaphore = asyncio.Semaphore(max_concurrency)

        self.enable_dom_xss = enable_dom_xss
        self.disable_csrf_check = disable_csrf_check
        self.max_depth = max_depth

        self.exclude_urls = [p.lower() for p in (exclude_urls or [])]
        self.headers = {}
        if user_agent:
            self.headers["User-Agent"] = user_agent

        self.visited_urls: Set[str] = set()
        self.results: List[str] = []

        # Queue for BFS-based scanning: holds (url, depth)
        self.url_queue: asyncio.Queue[Tuple[str, int]] = asyncio.Queue()
        self.url_queue.put_nowait((base_url, 0))

        self.append_log = log_callback or (lambda msg: None)

    def is_same_domain(self, url: str) -> bool:
        """Check if `url` is within the same domain as `self.base_url`."""
        return urlparse(url).netloc == urlparse(self.base_url).netloc

    def should_exclude(self, url: str) -> bool:
        """Return True if the URL should be excluded based on patterns."""
        return any(pattern in url.lower() for pattern in self.exclude_urls)

    async def fetch(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str = "GET",
        data: Optional[Dict[str, str]] = None,
        timeout: int = 10,
    ) -> Tuple[Optional[str], Optional[int]]:
        """
        Fetch URL using aiohttp, with concurrency managed by semaphore.
        Returns (text, status_code); (None, None) on failure.
        """
        async with self.semaphore:
            try:
                if method.upper() == "POST":
                    async with session.post(url, data=data, timeout=timeout) as resp:
                        text = await resp.text()
                        return text, resp.status
                else:
                    async with session.get(url, params=data, timeout=timeout) as resp:
                        text = await resp.text()
                        return text, resp.status
            except Exception as e:
                self.append_log(f"[DEBUG] Error fetching {url}: {e}")
                return None, None

    async def crawl_url(
        self, session: aiohttp.ClientSession, url: str, depth: int
    ) -> None:
        """Crawl a single URL: parse forms, test vulnerabilities, discover new links."""
        if url in self.visited_urls:
            return
        self.visited_urls.add(url)

        self.append_log(f"[*] Crawling (depth={depth}): {url}")
        html, status = await self.fetch(session, url)
        if not html or status != 200:
            self.append_log(f"[DEBUG] Skipping URL {url}, status={status}")
            return

        soup = BeautifulSoup(html, "html.parser")

        # 1. Analyze forms & test for vulnerabilities
        await self.test_forms(session, url, soup)

        # 2. CSRF checks if not disabled
        if not self.disable_csrf_check:
            self.check_csrf_tokens(url, soup)

        # 3. Discover new links if within depth limit
        if depth < self.max_depth:
            self.discover_links(soup, url, depth + 1)

    def discover_links(
        self, soup: BeautifulSoup, base_url: str, next_depth: int
    ) -> None:
        """Discover and enqueue new links from the page for further crawling."""
        for link in soup.find_all("a", href=True):
            full_link = urljoin(base_url, link["href"])
            if (
                self.is_same_domain(full_link)
                and not self.should_exclude(full_link)
                and full_link not in self.visited_urls
            ):
                self.append_log(
                    f"[DEBUG] Discovered link: {full_link} (depth={next_depth})"
                )
                self.url_queue.put_nowait((full_link, next_depth))

    def check_csrf_tokens(self, page_url: str, soup: BeautifulSoup) -> None:
        """
        Basic check for CSRF tokens in forms:
        - Looks for hidden inputs with 'csrf' in their name attribute.
        """
        for form in soup.find_all("form"):
            hidden_inputs = form.find_all("input", {"type": "hidden"})
            csrf_tokens = [
                inp
                for inp in hidden_inputs
                if re.search("csrf", inp.get("name", ""), re.IGNORECASE)
            ]
            if not csrf_tokens:
                msg = f"[!] Potential missing CSRF token on {page_url} (action={form.get('action')})"
                self.append_log(msg)
                self.results.append(msg)

    async def test_forms(
        self,
        session: aiohttp.ClientSession,
        page_url: str,
        soup: BeautifulSoup,
    ) -> None:
        """
        Extract forms, gather inputs, and submit payloads for SQLi, XSS, and other attacks.
        """
        for form in soup.find_all("form"):
            action = form.get("action") or ""
            method = form.get("method", "GET").upper()
            target_url = urljoin(page_url, action)

            inputs = form.find_all(["input", "textarea"])
            input_names = [inp.get("name") for inp in inputs if inp.get("name")]

            # Test each category of payload
            for category, payloads in [
                ("SQL Injection", SQL_PAYLOADS),
                ("XSS", XSS_PAYLOADS),
                ("Other Injection", OTHER_PAYLOADS),
            ]:
                for payload in payloads:
                    form_data = {name: payload for name in input_names}
                    await self.submit_form(
                        session, target_url, method, form_data, category
                    )

    async def submit_form(
        self,
        session: aiohttp.ClientSession,
        url: str,
        method: str,
        data: Dict[str, str],
        vuln_type: str,
    ) -> None:
        """Submit a form with a payload and perform checks for vulnerability signatures."""
        html, status = await self.fetch(session, url, method=method, data=data)
        if not html or status is None:
            return

        if vuln_type == "SQL Injection":
            for signature in SQL_ERROR_SIGNATURES:
                if signature.lower() in html.lower():
                    msg = f"[!] Possible SQL Injection at {url} with payload={data}"
                    self.append_log(msg)
                    self.results.append(msg)
                    return

        elif vuln_type == "XSS":
            # Check if payload is reflected in the response
            for val in data.values():
                if val in html:
                    msg = f"[!] Possible Reflected XSS at {url} with payload='{val}'"
                    self.append_log(msg)
                    self.results.append(msg)
                    return

    async def detect_dom_xss(self) -> None:
        """
        Use Playwright to detect DOM-based XSS by injecting payloads as query parameters.
        """
        if not async_playwright:
            self.append_log(
                "[ERROR] Playwright is not installed. Skipping DOM-based XSS."
            )
            return

        candidate_urls = [u for u in self.visited_urls if self.is_same_domain(u)]
        self.append_log("[+] Starting DOM-based XSS detection...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()

            for url in candidate_urls:
                for payload in XSS_PAYLOADS:
                    test_url = self.append_payload_to_url(url, payload)
                    self.append_log(f"[DEBUG] Testing DOM-based XSS on {test_url}")

                    page = await context.new_page()

                    async def on_dialog(dialog):
                        msg = f"[!] Possible DOM XSS at {test_url}. Dialog text: {dialog.message}"
                        self.append_log(msg)
                        self.results.append(msg)
                        await dialog.dismiss()

                    page.on("dialog", on_dialog)

                    try:
                        await page.goto(test_url, timeout=10000)
                        # Let scripts run briefly
                        await asyncio.sleep(2)
                    except Exception as e:
                        self.append_log(
                            f"[DEBUG] DOM XSS test failed at {test_url}: {e}"
                        )
                    finally:
                        await page.close()

            await context.close()
            await browser.close()

    @staticmethod
    def append_payload_to_url(url: str, payload: str) -> str:
        """Append an XSS payload as a query parameter for DOM XSS testing."""
        if "?" in url:
            return f"{url}&xss={payload}"
        return f"{url}?xss={payload}"

    async def run(self) -> List[str]:
        """
        Main scan loop: connect, crawl all pages in BFS manner, optionally check DOM XSS.
        Returns the list of discovered vulnerabilities.
        """
        conn = aiohttp.TCPConnector(ssl=False)  # Optionally ignore SSL for testing
        async with aiohttp.ClientSession(
            connector=conn, headers=self.headers
        ) as session:
            while not self.url_queue.empty():
                url, depth = await self.url_queue.get()
                await self.crawl_url(session, url, depth)

        if self.enable_dom_xss:
            await self.detect_dom_xss()

        return self.results


# --------------------------------------------------
# Typer CLI Definition
# --------------------------------------------------
app = typer.Typer(help="Async Python Web Vulnerability Scanner (Typer + curses)")

console = Console()


@app.command()
def scan(
    base_url: str = typer.Option(
        ...,
        prompt="Enter the target URL (e.g. https://example.com)",
        help="The starting URL to scan",
    ),
    max_concurrency: int = typer.Option(
        5,
        prompt="Max concurrency?",
        help="Number of concurrent requests to send at once",
    ),
    enable_dom_xss: bool = typer.Option(
        False,
        prompt="Enable DOM-based XSS checks?",
        help="If True, uses Playwright to detect DOM XSS",
    ),
    disable_csrf_check: bool = typer.Option(
        False,
        prompt="Disable CSRF checks?",
        help="If True, CSRF token checks are skipped",
    ),
    max_depth: int = typer.Option(
        3,
        prompt="Max crawl depth?",
        help="Limits how deep the crawler will follow links",
    ),
    exclude_patterns: str = typer.Option(
        "",
        prompt="Comma-separated URL substrings to exclude? (leave blank if none)",
        help="Exclude URLs containing any of these substrings",
    ),
    user_agent: str = typer.Option(
        "",
        prompt="Custom User-Agent? (leave blank for default)",
        help="Specify a custom User-Agent header",
    ),
):
    """
    Launches an asynchronous vulnerability scan of the target web application.
    """
    # Convert comma-separated excludes to a list
    exclude_list = []
    if exclude_patterns.strip():
        exclude_list = [x.strip() for x in exclude_patterns.split(",") if x.strip()]

    def log_callback(msg: str) -> None:
        # Print logs to the Rich console for colorized real-time feedback
        console.print(msg)

    # Basic validation
    if not base_url.lower().startswith("http"):
        console.print(
            "[bold red][ERROR][/bold red] Please provide a valid URL starting with http or https."
        )
        raise typer.Exit(code=1)

    console.print(f"[bold cyan]Starting scan for:[/bold cyan] {base_url}")
    console.print(f"   Concurrency: [yellow]{max_concurrency}[/yellow]")
    console.print(f"   Max Depth: [yellow]{max_depth}[/yellow]")
    console.print(f"   Enable DOM XSS: [yellow]{enable_dom_xss}[/yellow]")
    console.print(f"   Disable CSRF Checks: [yellow]{disable_csrf_check}[/yellow]")
    console.print(f"   Exclude Patterns: [yellow]{exclude_list}[/yellow]")
    if user_agent:
        console.print(f"   Custom User-Agent: [yellow]{user_agent}[/yellow]")

    scanner = AsyncWebVulnerabilityScanner(
        base_url=base_url,
        max_concurrency=max_concurrency,
        enable_dom_xss=enable_dom_xss,
        disable_csrf_check=disable_csrf_check,
        max_depth=max_depth,
        exclude_urls=exclude_list,
        user_agent=user_agent if user_agent else None,
        log_callback=log_callback,
    )

    # Run the asynchronous scanner
    vulnerabilities = asyncio.run(scanner.run())

    console.print("[bold cyan]\nScan complete. Results:[/bold cyan]\n")

    if vulnerabilities:
        for finding in vulnerabilities:
            console.print(f"  - [red]{finding}[/red]")
    else:
        console.print("[green]No major findings reported.[/green]")


@app.command()
def tui():
    """
    Explicitly launch the curses-based TUI for interactive vulnerability scanning.
    """
    curses.wrapper(main_tui)


# ----------------------------------------
# Curses-based TUI Function
# ----------------------------------------
def main_tui(stdscr: "curses._CursesWindow") -> None:
    """
    A very simple curses TUI that interacts with the user to gather scan parameters,
    then runs the same async scanning process. Feel free to customize more extensively!
    """

    curses.curs_set(1)  # Show the cursor for user input
    stdscr.clear()

    # Instructions
    instructions = [
        "Asynchronous Web Vulnerability Scanner (TUI Mode)",
        "Fill out the parameters below and press ENTER to start scanning.",
        "Press Ctrl+C at any time to quit.\n",
    ]

    # Parameter prompts
    fields = [
        ("Base URL (e.g., https://example.com)", ""),
        ("Max concurrency", "5"),
        ("Enable DOM XSS? (True/False)", "False"),
        ("Disable CSRF check? (True/False)", "False"),
        ("Max depth", "3"),
        ("Exclude patterns (comma-separated)", ""),
        ("Custom User-Agent", ""),
    ]

    # Print instructions
    y = 0
    for line in instructions:
        stdscr.addstr(y, 0, line)
        y += 1
    y += 1

    # A simple line editor for each field
    inputs = []
    for i, (prompt, default_val) in enumerate(fields):
        stdscr.addstr(y, 0, f"{prompt}: ")
        curses.echo()
        stdscr.refresh()
        user_val = stdscr.getstr(y, len(prompt) + 2, 60).decode("utf-8").strip()
        curses.noecho()
        if not user_val:
            user_val = default_val
        inputs.append(user_val)
        y += 1

    # Unpack fields
    base_url = inputs[0]
    max_concurrency = int(inputs[1])
    enable_dom_xss = inputs[2].lower() == "true"
    disable_csrf_check = inputs[3].lower() == "true"
    max_depth = int(inputs[4])
    exclude_patterns = inputs[5]
    user_agent = inputs[6].strip() if inputs[6] else None

    exclude_list = []
    if exclude_patterns.strip():
        exclude_list = [x.strip() for x in exclude_patterns.split(",") if x.strip()]

    # Prepare a log callback that writes to the curses window
    log_y = y + 2

    def log_callback(msg: str) -> None:
        nonlocal log_y
        # If screen starts to overflow, scroll up or adjust as needed
        if log_y >= curses.LINES - 1:
            stdscr.scroll(1)
            log_y -= 1
        stdscr.addstr(log_y, 0, msg)
        stdscr.refresh()
        log_y += 1

    # Basic validation
    if not base_url.lower().startswith("http"):
        log_callback("[ERROR] Please provide a valid URL (start with http or https).")
        stdscr.getch()
        return

    # Summarize scanning parameters in TUI
    log_callback(f"[INFO] Starting scan for: {base_url}")
    log_callback(f"       Concurrency: {max_concurrency}")
    log_callback(f"       Max Depth: {max_depth}")
    log_callback(f"       Enable DOM XSS: {enable_dom_xss}")
    log_callback(f"       Disable CSRF Checks: {disable_csrf_check}")
    log_callback(f"       Exclude Patterns: {exclude_list}")
    if user_agent:
        log_callback(f"       Custom User-Agent: {user_agent}")

    scanner = AsyncWebVulnerabilityScanner(
        base_url=base_url,
        max_concurrency=max_concurrency,
        enable_dom_xss=enable_dom_xss,
        disable_csrf_check=disable_csrf_check,
        max_depth=max_depth,
        exclude_urls=exclude_list,
        user_agent=user_agent if user_agent else None,
        log_callback=log_callback,
    )

    # Run the asynchronous scanner in this TUI
    vulnerabilities = asyncio.run(scanner.run())

    # Print results
    log_callback("\n[INFO] Scan complete. Results:")
    if vulnerabilities:
        for finding in vulnerabilities:
            log_callback(f"  - {finding}")
    else:
        log_callback("No major findings reported.")

    stdscr.addstr(log_y + 2, 0, "Press any key to exit...")
    stdscr.refresh()
    stdscr.getch()


# ----------------------------------------
# Main Entry Point
# ----------------------------------------
def main():
    """
    Launch TUI if no arguments or first arg == tui. Otherwise, run Typer CLI.
    """
    if len(sys.argv) == 1:
        curses.wrapper(main_tui)
    elif sys.argv[1].lower() == "tui":
        curses.wrapper(main_tui)
    else:
        app()


if __name__ == "__main__":
    main()
